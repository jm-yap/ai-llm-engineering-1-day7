{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxpWDFG11o3G"
      },
      "source": [
        "# Multi-Agent Workflows + RAG - LangGraph\n",
        "\n",
        "Today we'll be looking at an example of a Multi-Agent workflow that's powered by LangGraph, LCEL, and more!\n",
        "\n",
        "We're going to be, more specifically, looking at a \"heirarchical agent teams\" from the [AutoGen: Enabling Next-Gen LLM\n",
        "Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155) paper.\n",
        "\n",
        "This will be the final \"graph\" of our system:\n",
        "\n",
        "![image](https://i.imgur.com/Xro0QiR.png)\n",
        "\n",
        "It's important to keep in mind that the actual implementation will be constructed of 3 separate graphs, the final one having 2 graphs as nodes! LangGraph is a heckuva tool!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyzoBrWoYeOZ"
      },
      "source": [
        "# 🤝 BREAKOUT ROOM #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3oaVoX5cA2"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpv2MWqu5vS9"
      },
      "source": [
        "Since we'll be relying on OpenAI's suite of models to power our agents today, we'll want to provide our OpenAI API Key.\n",
        "\n",
        "We're also going to be using the Tavily search tool - so we'll want to provide that API key as well!\n",
        "\n",
        "Instruction for how to obtain the Tavily API key can be found:\n",
        "\n",
        "1. [Tavily API Key](https://app.tavily.com/sign-in)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h30OjkLfeR2Y",
        "outputId": "f75bb26e-b89d-4611-c29b-f339b3e868af"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"PSI Day7 - AI Bills - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_LD7rwT6PbO"
      },
      "source": [
        "## Task 1: Simple LangGraph RAG\n",
        "\n",
        "Now that we have our dependencies set-up - let's create a simple RAG graph that works over our Loan PDFs from previous sessions.\n",
        "\n",
        "> NOTE: While this particular example is very straight forward - you can \"plug in\" any complexity of chain you desire as a node in a LangGraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY7T5kxJ6jGn"
      },
      "source": [
        "## Retrieval\n",
        "\n",
        "The 'R' in 'RAG' - this is, at this point, fairly straightforward!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGuPxSCk7Ztz"
      },
      "source": [
        "#### Data Collection and Processing\n",
        "\n",
        "A classic first step, at this point, let's grab our desired document!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LfuoEYRCln3H"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(\"bills\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "\n",
        "bill_knowledge_resources = directory_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_t_F1zG6vXa"
      },
      "source": [
        "Now we can chunk it down to size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5R7A_z8CgL79"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "bill_knowledge_chunk = text_splitter.split_documents(bill_knowledge_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGE-VuMc7AKv"
      },
      "source": [
        "Now we've successfully split our single PDF into..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgYBHsdWmLvW",
        "outputId": "aa9a830e-f7db-4bb3-f542-c0614cb01aca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(bill_knowledge_chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxaKmmyh7DHD"
      },
      "source": [
        "documents!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGWs7KTd7QPS"
      },
      "source": [
        "#### Embedding Model and Vector Store\n",
        "\n",
        "Now that we have our chunked document - lets create a vector store, which will first require us to create an embedding model to get the vector representations of our text!\n",
        "\n",
        "We'll use OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) model - as it's cheap, and performant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xLIWMMZCmfrj"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTEi7Ww573sc"
      },
      "source": [
        "Now we can create our QDrant backed vector store!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xct51f8omVAU"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=bill_knowledge_chunk,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzGq6o4s79Ar"
      },
      "source": [
        "Let's make sure we can access it as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OTnQZbWymi4K"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU8qSrMS7_D7"
      },
      "source": [
        "### Augmented\n",
        "\n",
        "Now that we have our retrieval process set-up, we need to set up our \"augmentation\" process - AKA a prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lezTN0zCmk46"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9fa63nM7IKK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Last, but certainly not least, let's put the 'G' in 'RAG' by adding our generator - in this case, we can rely on OpenAI's [`gpt-4o-mini`](https://platform.openai.com/docs/models/gpt-4o-mini) model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AwEi29-Jo3a8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-5-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO-ZC0T98XJJ"
      },
      "source": [
        "### RAG - Retrieval Augmented Generation\n",
        "\n",
        "All that's left to do is combine our R, A, and G into a single graph - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nlOJrPm_oT3S"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'ConfiguredManagedValue' from 'langgraph.managed.base' (/Users/jela.yap/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/managed/base.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m START, StateGraph\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/graph/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m END, START, Graph\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MessageGraph, MessagesState, add_messages\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/graph/graph.py:37\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     END,\n\u001b[32m     30\u001b[39m     NS_END,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     Send,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InvalidUpdateError\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Channel, Pregel\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mread\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PregelNode\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m All\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:77\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphInterrupt, GraphRecursionError, InvalidUpdateError\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanaged\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ManagedValueSpec\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     78\u001b[39m     apply_writes,\n\u001b[32m     79\u001b[39m     local_read,\n\u001b[32m     80\u001b[39m     local_write,\n\u001b[32m     81\u001b[39m     prepare_next_tasks,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch_checkpoint_map, patch_configurable\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     85\u001b[39m     print_step_checkpoint,\n\u001b[32m     86\u001b[39m     print_step_tasks,\n\u001b[32m     87\u001b[39m     print_step_writes,\n\u001b[32m     88\u001b[39m     tasks_w_writes,\n\u001b[32m     89\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/pregel/algo.py:51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_channel, read_channels\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlog\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelsManager\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mread\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PregelNode\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m All, PregelExecutableTask, PregelTask\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/pregel/manager.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_KEY_STORE\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanaged\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     ConfiguredManagedValue,\n\u001b[32m     12\u001b[39m     ManagedValueMapping,\n\u001b[32m     13\u001b[39m     ManagedValueSpec,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanaged\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Context\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseStore\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'ConfiguredManagedValue' from 'langgraph.managed.base' (/Users/jela.yap/Desktop/projects/psi-ai/day_7/.venv/lib/python3.11/site-packages/langgraph/managed/base.py)"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: list[Document]\n",
        "  response: str\n",
        "\n",
        "def retrieve(state: State) -> State:\n",
        "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}\n",
        "\n",
        "def generate(state: State) -> State:\n",
        "  generator_chain = chat_prompt | openai_chat_model | StrOutputParser()\n",
        "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
        "  return {\"response\" : response}\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "rag_graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiWrbXpu8ggz"
      },
      "source": [
        "Let's test this out and make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gJhFlW32pBPe",
        "outputId": "7aee04b6-608f-4639-adca-66225d4d3002"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "result = rag_graph.invoke({\"question\" : \"What is the best approach for AI in education?\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, textwrap, dataclasses\n",
        "from typing import Any\n",
        "\n",
        "def pretty_print(\n",
        "    obj: Any,\n",
        "    *,\n",
        "    width: int = 100,          # target wrap width for long strings\n",
        "    indent: int = 2,           # JSON indent\n",
        "    sort_keys: bool = False,   # sort dict keys\n",
        "    max_str: int = 800,        # truncate very long strings\n",
        "    max_list: int = 200,       # cap huge lists\n",
        "    wrap_strings: bool = True, # wrap long strings\n",
        "):\n",
        "    def is_langchain_doc(o):\n",
        "        return o.__class__.__name__ == \"Document\" and hasattr(o, \"page_content\")\n",
        "\n",
        "    def to_serializable(o, _depth=0):\n",
        "        # LangChain Document → compact dict\n",
        "        if is_langchain_doc(o):\n",
        "            meta = getattr(o, \"metadata\", {}) or {}\n",
        "            # keep only commonly useful metadata keys (expand as needed)\n",
        "            keep = [\"source\", \"file_path\", \"page\", \"title\", \"author\"]\n",
        "            filtered_meta = {k: meta.get(k) for k in keep if k in meta}\n",
        "            return {\n",
        "                \"type\": \"Document\",\n",
        "                \"page_content\": o.page_content,\n",
        "                \"metadata\": filtered_meta or meta,  # fallback to all if nothing matched\n",
        "            }\n",
        "\n",
        "        # dataclasses\n",
        "        if dataclasses.is_dataclass(o):\n",
        "            return {k: to_serializable(v, _depth+1) for k, v in dataclasses.asdict(o).items()}\n",
        "\n",
        "        # NumPy / pandas scalars\n",
        "        try:\n",
        "            import numpy as np  # optional\n",
        "            if isinstance(o, (np.generic,)):\n",
        "                return o.item()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # bytes → hex (short) or utf-8 (best effort)\n",
        "        if isinstance(o, (bytes, bytearray)):\n",
        "            try:\n",
        "                return o.decode(\"utf-8\")\n",
        "            except Exception:\n",
        "                return o[:16].hex() + (\"…hex\" if len(o) > 16 else \"\")\n",
        "\n",
        "        # primitives\n",
        "        if isinstance(o, (str, int, float, bool)) or o is None:\n",
        "            return o\n",
        "\n",
        "        # dicts\n",
        "        if isinstance(o, dict):\n",
        "            return {str(k): to_serializable(v, _depth+1) for k, v in o.items()}\n",
        "\n",
        "        # lists / tuples / sets\n",
        "        if isinstance(o, (list, tuple, set)):\n",
        "            seq = list(o)\n",
        "            if len(seq) > max_list:\n",
        "                seq = seq[:max_list] + [f\"…({len(o)-max_list} more)\"]\n",
        "            return [to_serializable(v, _depth+1) for v in seq]\n",
        "\n",
        "        # fallback: try __dict__, else string repr\n",
        "        return to_serializable(getattr(o, \"__dict__\", str(o)), _depth+1)\n",
        "\n",
        "    def transform_strings(x):\n",
        "        # truncate + wrap strings inside the structure\n",
        "        if isinstance(x, str):\n",
        "            s = x if max_str is None or len(x) <= max_str else (x[:max_str] + \"…\")\n",
        "            if wrap_strings and width:\n",
        "                s = \"\\n\".join(textwrap.wrap(s, width=width)) if \"\\n\" not in s else \\\n",
        "                    \"\\n\".join(\"\\n\".join(textwrap.wrap(line, width=width)) for line in s.splitlines())\n",
        "            return s\n",
        "        if isinstance(x, list):\n",
        "            return [transform_strings(v) for v in x]\n",
        "        if isinstance(x, dict):\n",
        "            return {k: transform_strings(v) for k, v in x.items()}\n",
        "        return x\n",
        "\n",
        "    normalized = to_serializable(obj)\n",
        "    normalized = transform_strings(normalized)\n",
        "    print(json.dumps(normalized, indent=indent, ensure_ascii=False, sort_keys=sort_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretty_print(result, width=100, max_str=1200, sort_keys=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(result[\"response\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gReMizYk8qd-"
      },
      "source": [
        "### RAG Limitation\n",
        "\n",
        "Notice how we're hard-coding our data, while this is simply meant to be an illustrative example - you could easily extend this to work with any provied paper or document in order to have a more dynamic system.\n",
        "\n",
        "For now, we'll stick with this single hard-coded example in order to keep complexity down in an already very long notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxkbuir-H5rE"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "##### 🏗️ Activity #1\n",
        "\n",
        "Allow the system to dynamically fetch Arxiv papers instead of hard coding them.\n",
        "\n",
        "> HINT: Tuesday's assignment will be very useful here.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import arxiv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.tools import tool\n",
        "from typing import Annotated\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "@tool\n",
        "def fetch_arxiv_paper(\n",
        "    query: Annotated[str, \"Search query for arxiv papers\"],\n",
        "    max_results: Annotated[int, \"Maximum number of papers to fetch\"] = 1\n",
        ") -> Annotated[str, \"Status message about fetching papers\"]:\n",
        "    \"\"\"Fetch and process arxiv papers dynamically based on query.\"\"\"\n",
        "    try:\n",
        "        # Search for papers on arxiv\n",
        "        client = arxiv.Client()\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=max_results,\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "        \n",
        "        papers_processed = 0\n",
        "        for paper in client.results(search):\n",
        "            # Download the paper\n",
        "            paper_path = paper.download_pdf(dirpath=tempfile.gettempdir())\n",
        "            \n",
        "            # Load and process the PDF\n",
        "            loader = PyPDFLoader(paper_path)\n",
        "            documents = loader.load()\n",
        "            \n",
        "            # Add to our existing vector store\n",
        "            global qdrant_vectorstore\n",
        "            qdrant_vectorstore.add_documents(documents)\n",
        "            \n",
        "            papers_processed += 1\n",
        "            \n",
        "            # Clean up the downloaded file\n",
        "            os.remove(paper_path)\n",
        "        \n",
        "        return f\"Successfully fetched and processed {papers_processed} papers for query: '{query}'\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error fetching papers: {str(e)}\"\n",
        "\n",
        "# Enhanced RAG tool that can dynamically fetch papers\n",
        "@tool\n",
        "def enhanced_retrieve_information(\n",
        "    query: Annotated[str, \"query to retrieve information\"],\n",
        "    fetch_new_papers: Annotated[bool, \"whether to fetch new papers from arxiv\"] = False,\n",
        "    arxiv_query: Annotated[str, \"arxiv search query if fetching new papers\"] = \"\"\n",
        "    ):\n",
        "    \"\"\"Enhanced retrieval with dynamic arxiv paper fetching capability.\"\"\"\n",
        "    \n",
        "    if fetch_new_papers and arxiv_query:\n",
        "        fetch_status = fetch_arxiv_paper.invoke({\"query\": arxiv_query, \"max_results\": 2})\n",
        "        print(f\"Arxiv fetch status: {fetch_status}\")\n",
        "    \n",
        "    # Use existing RAG system\n",
        "    return rag_graph.invoke({\"question\": query})\n",
        "\n",
        "# Example usage:\n",
        "# To use this in your agent, you would replace the basic retrieve_information tool\n",
        "# with this enhanced version that can dynamically fetch relevant papers\n",
        "\n",
        "# Test the enhanced system\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: Fetch papers about AI in education and then query\n",
        "    result = enhanced_retrieve_information.invoke({\n",
        "        \"query\": \"What are the latest approaches to AI in education?\",\n",
        "        \"fetch_new_papers\": True,\n",
        "        \"arxiv_query\": \"artificial intelligence education\"\n",
        "    })\n",
        "    print(\"Enhanced RAG Result:\")\n",
        "    print(result[\"response\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U6a_pqQ9uWf"
      },
      "source": [
        "## Task 2: Helper Functions for Agent Graphs\n",
        "\n",
        "We'll be using a number of agents, nodes, and supervisors in the rest of the notebook - and so it will help to have a collection of useful helper functions that we can leverage to make our lives easier going forward.\n",
        "\n",
        "Let's start with the most simple one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDUnpEEl-L_F"
      },
      "source": [
        "#### Import Wall\n",
        "\n",
        "Here's a wall of imports we'll be needing going forward!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbzoL3Q3-SG1"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.tools import BaseTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langgraph.graph import END, StateGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb6Z3EEz-Asi"
      },
      "source": [
        "### Agent Node Helper\n",
        "\n",
        "Since we're going to be wrapping each of our agents into a node - it will help to have an easy way to create the node!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IF7KWfS-JKd"
      },
      "outputs": [],
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwND2teK-WHm"
      },
      "source": [
        "### Agent Creation Helper Function\n",
        "\n",
        "Since we know we'll need to create agents to populate our agent nodes, let's use a helper function for that as well!\n",
        "\n",
        "Notice a few things:\n",
        "\n",
        "1. We have a standard suffix to append to our system messages for each agent to handle the tool calling and boilerplate prompting.\n",
        "2. Each agent has its our scratchpad.\n",
        "3. We're relying on OpenAI's function-calling API for tool selection\n",
        "4. Each agent is its own executor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxLyHJt5-eUx"
      },
      "outputs": [],
      "source": [
        "def create_agent(\n",
        "    llm: ChatOpenAI,\n",
        "    tools: list,\n",
        "    system_prompt: str,\n",
        ") -> str:\n",
        "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
        "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
        "    \" Do not ask for clarification.\"\n",
        "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
        "    \" You are chosen for a reason!\")\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6kmlR9d-1K5"
      },
      "source": [
        "### Supervisor Helper Function\n",
        "\n",
        "Finally, we need a \"supervisor\" that decides and routes tasks to specific agents.\n",
        "\n",
        "Since each \"team\" will have a collection of potential agents - this \"supervisor\" will act as an \"intelligent\" router to make sure that the right agent is selected for the right task.\n",
        "\n",
        "Notice that, at the end of the day, this \"supervisor\" is simply directing who acts next - or if the state is considered \"done\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2MXA83mrYE2"
      },
      "outputs": [],
      "source": [
        "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
        "    \"\"\"An LLM-based router.\"\"\"\n",
        "    options = [\"FINISH\"] + members\n",
        "    function_def = {\n",
        "        \"name\": \"route\",\n",
        "        \"description\": \"Select the next role.\",\n",
        "        \"parameters\": {\n",
        "            \"title\": \"routeSchema\",\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"next\": {\n",
        "                    \"title\": \"Next\",\n",
        "                    \"anyOf\": [\n",
        "                        {\"enum\": options},\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"next\"],\n",
        "        },\n",
        "    }\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Given the conversation above, who should act next?\"\n",
        "                \" Or should we FINISH? Select one of: {options}\",\n",
        "            ),\n",
        "        ]\n",
        "    ).partial(options=str(options), team_members=\", \".join(members))\n",
        "    return (\n",
        "        prompt\n",
        "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "        | JsonOutputFunctionsParser()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd0zfyq48jKb"
      },
      "source": [
        "## Task 3: Research Team - A LangGraph for Researching on AI Proposed Bills\n",
        "\n",
        "Now that we have our RAG chain set-up and some awesome helper functions, we want to create a LangGraph related to researching a specific topic, in this case: Loans!\n",
        "\n",
        "We're going to start by equipping our Research Team with a few tools:\n",
        "\n",
        "1. Tavily Search - aka \"Google\", for the most up to date information possible.\n",
        "2. Our RAG chain - specific and high quality information about our topic.\n",
        "\n",
        "Let's create those tools now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNsVTZrH_alw"
      },
      "source": [
        "### Tool Creation\n",
        "\n",
        "As you can see below, some tools already come pre-packaged ready to use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce7FKTZDgAWG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIR7cbTL9agM"
      },
      "source": [
        "Creating a custom tool, however, is very straightforward.\n",
        "\n",
        "> NOTE: You *must* include a docstring, as that is what the LLM will consider when deciding when to use this tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSwO2L_UqFhm"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, List, Tuple, Union\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def retrieve_information(\n",
        "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
        "    ):\n",
        "  \"\"\"Use Retrieval Augmented Generation to retrieve information about the AI bills proposed in the Philippines.\"\"\"\n",
        "  return rag_graph.invoke({\"question\" : query})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsMnqjpBTCj"
      },
      "source": [
        "> NOTE: We could just as easily use the LCEL chain directly, since nodes can be LCEL objects - but creating a tool helps explain the tool creation process at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHCajO4_gB2"
      },
      "source": [
        "### Research Team State\n",
        "\n",
        "Since we're using LangGraph - we're going to need state!\n",
        "\n",
        "Let's look at how we've created our state below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXminK9d_1fa"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import operator\n",
        "\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "import functools\n",
        "\n",
        "class ResearchTeamState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: List[str]\n",
        "    next: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvPM5msq_18C"
      },
      "source": [
        "Notice how we've used `messages`, `team_members`, and `next`.\n",
        "\n",
        "These states will help us understand:\n",
        "\n",
        "1. What we've done so far (`messages`)\n",
        "2. Which team members we have access to (`team_members`)\n",
        "3. Which team member is up next! (`next`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu7B_6qHAFjK"
      },
      "source": [
        "### Research Team LLM\n",
        "\n",
        "We'll be using `gpt-5o-mini` today. This LLM is going to be doing a lot of reasoning - but we also want to keep our costs down, so we'll use a lightweight; but powerful, model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTNqrip8AcKR"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfb_VCNKIy9w"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "##### ❓ Question #1:\n",
        "\n",
        "Why is a \"powerful\" LLM important for this use-case?\n",
        "\n",
        "What tasks must our Agent perform that make it such that the LLM's reasoning capability is a potential limiter?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer\n",
        "\n",
        "A powerful LLM is essential for multi-agent workflows because agents rely on advanced reasoning to select and sequence tools, interpret nuanced instructions, synthesize information, and coordinate with one another. Strong capabilities are also needed for dynamic decision-making, adapting to unexpected results, and ensuring quality control across multi-step processes. Without sufficient reasoning power, weaker models risk poor tool choices, context loss, misrouted tasks, and cascading errors. This makes models like GPT-4o-mini valuable, as they balance strong reasoning with cost efficiency for frequent LLM calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_1LuMKAekf"
      },
      "source": [
        "### Research Team Agents & Nodes\n",
        "\n",
        "Now we can use our helper functions to create our agent nodes, with their related tools.\n",
        "\n",
        "Let's start with our search agent node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzx6wuPoAlPq"
      },
      "source": [
        "#### Research Team: Search Agent\n",
        "\n",
        "We're going to give our agent access to the Tavily tool, power it with our GPT-4o Mini model, and then create its node - and name it `Search`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIlLPxj7Atpj"
      },
      "outputs": [],
      "source": [
        "search_agent = create_agent(\n",
        "    llm,\n",
        "    [tavily_tool],\n",
        "    \"You are a research assistant who can search for latest info using the tavily search engine. Do not use on AI bills.\",\n",
        ")\n",
        "search_node = functools.partial(agent_node, agent=search_agent, name=\"Search\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emLtesudA9Dd"
      },
      "source": [
        "#### Research Team: RAG Agent Node\n",
        "\n",
        "Now we can wrap our LCEL RAG pipeline in an agent node as well, using the LCEL RAG pipeline as the tool, as created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-nnAG9XA_p7"
      },
      "outputs": [],
      "source": [
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [retrieve_information],\n",
        "    \"You are a research assistant who can provide information on the AI bills proposed in the Philippines.\",\n",
        ")\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"BillRetriever\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA5z6T1CBeSc"
      },
      "source": [
        "### Research Team Supervisor Agent\n",
        "\n",
        "Notice that we're not yet creating our supervisor *node*, simply the agent here.\n",
        "\n",
        "Also notice how we need to provide a few extra pieces of information - including which tools we're using.\n",
        "\n",
        "> NOTE: It's important to use the *exact* tool name, as that is how the LLM will reference the tool. Also, it's important that your tool name is all a single alphanumeric string!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0g8CQMBrtFs"
      },
      "outputs": [],
      "source": [
        "supervisor_agent = create_team_supervisor(\n",
        "    llm,\n",
        "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers:  Search, BillRetriever. Given the following user request,\"\n",
        "    \" determine the subject to be researched and respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. \"\n",
        "    \" You should never ask your team to do anything beyond research. They are not required to write content or posts.\"\n",
        "    \" You should only pass tasks to workers that are specifically research focused.\"\n",
        "    \" When finished, respond with FINISH.\"),\n",
        "    [\"Search\", \"BillRetriever\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qohn0DcgB_U1"
      },
      "source": [
        "### Research Team Graph Creation\n",
        "\n",
        "Now that we have our research team agent nodes created, and our supervisor agent - let's finally construct our graph!\n",
        "\n",
        "We'll start by creating our base graph from our state, and then adding the nodes/agent we've created as nodes on our LangGraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0s2GAgJCN8G"
      },
      "outputs": [],
      "source": [
        "research_graph = StateGraph(ResearchTeamState)\n",
        "\n",
        "research_graph.add_node(\"Search\", search_node)\n",
        "research_graph.add_node(\"BillRetriever\", research_node)\n",
        "research_graph.add_node(\"supervisor\", supervisor_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33qixRGNCaAX"
      },
      "source": [
        "Now we can define our edges - include our conditional edge from our supervisor to our agent nodes.\n",
        "\n",
        "Notice how we're always routing our agent nodes back to our supervisor!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYSJIhijsGyg"
      },
      "outputs": [],
      "source": [
        "research_graph.add_edge(\"Search\", \"supervisor\")\n",
        "research_graph.add_edge(\"BillRetriever\", \"supervisor\")\n",
        "research_graph.add_conditional_edges(\n",
        "    \"supervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\"Search\": \"Search\", \"BillRetriever\": \"BillRetriever\", \"FINISH\": END},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgGcuZzkCj1-"
      },
      "source": [
        "Now we can set our supervisor node as the entry point, and compile our graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l-1I2Z3CnPX"
      },
      "outputs": [],
      "source": [
        "research_graph.set_entry_point(\"supervisor\")\n",
        "compiled_research_graph = research_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDwQpYTSEY13"
      },
      "source": [
        "#### Display Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "l8n6SXhpEa2b",
        "outputId": "6dac5e4e-daed-4d7a-d629-cd83119e7e2c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        compiled_research_graph.get_graph().draw_mermaid_png(\n",
        "            curve_style=CurveStyle.LINEAR,\n",
        "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
        "            wrap_label_n_words=9,\n",
        "            output_file_path=None,\n",
        "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
        "            background_color=\"white\",\n",
        "            padding=10,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfRvA2QfCqFL"
      },
      "source": [
        "The next part is key - since we need to \"wrap\" our LangGraph in order for it to be compatible in the following steps - let's create an LCEL chain out of it!\n",
        "\n",
        "This allows us to \"broadcast\" messages down to our Research Team LangGraph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G7hmEINCx3i"
      },
      "outputs": [],
      "source": [
        "def enter_chain(message: str):\n",
        "    results = {\n",
        "        \"messages\": [HumanMessage(content=message)],\n",
        "    }\n",
        "    return results\n",
        "\n",
        "research_chain = enter_chain | compiled_research_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGdoCdXWC7Pi"
      },
      "source": [
        "Now, finally, we can take it for a spin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIDpFIg2sRUl",
        "outputId": "bb3803d4-5b32-4b0a-c8a1-1a1917425812"
      },
      "outputs": [],
      "source": [
        "for s in research_chain.stream(\n",
        "    \"What is the maximum penalty for the violation of the AI bill?\", {\"recursion_limit\": 10}\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for s in research_chain.stream(\n",
        "    \"In Philippines, what is the maximum penalty for the violation of the AI bill?\", {\"recursion_limit\": 10}\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Question: Why Search?!?!\n",
        "\n",
        "The supervisor seems to prefer Search. \n",
        "How can you improve the system to prioritize billretriever?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAgsbwIIhwj"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Using whatever drawing application you wish - please label the flow above on a diagram of your graph.\n",
        "\n",
        "Hint: try asking Cursor agent to draw diagram using mermaid... sometimes you can get lucky.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Research Team Graph](research-team-graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color:teal; color: black; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### BREAK-OUT #2\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejsHCZZ2EmwM"
      },
      "source": [
        "## Task 4: Document Writing Team - A LangGraph for  Planning, Writing, and Editing Response.\n",
        "\n",
        "Let's run it all back, this time specifically creating tools, agent nodes, and a graph for Reviewing, Planning, Writing Response!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4awQtZ-oFUN-"
      },
      "source": [
        "### Tool Creation\n",
        "\n",
        "Let's create some tools that will help us understand, open, work with, and edit documents to our liking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptXilgparOkq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Dict, Optional\n",
        "from typing_extensions import TypedDict\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "os.makedirs('./content/data', exist_ok=True)\n",
        "\n",
        "def create_random_subdirectory():\n",
        "    random_id = str(uuid.uuid4())[:8]  # Use first 8 characters of a UUID\n",
        "    subdirectory_path = os.path.join('./content/data', random_id)\n",
        "    os.makedirs(subdirectory_path, exist_ok=True)\n",
        "    return subdirectory_path\n",
        "\n",
        "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
        "\n",
        "@tool\n",
        "def create_outline(\n",
        "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
        "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
        ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
        "    \"\"\"Create and save an outline.\"\"\"\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        for i, point in enumerate(points):\n",
        "            file.write(f\"{i + 1}. {point}\\n\")\n",
        "    return f\"Outline saved to {file_name}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def read_document(\n",
        "    file_name: Annotated[str, \"File path to save the document.\"],\n",
        "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
        "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
        ") -> str:\n",
        "    \"\"\"Read the specified document.\"\"\"\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    if start is not None:\n",
        "        start = 0\n",
        "    return \"\\n\".join(lines[start:end])\n",
        "\n",
        "@tool\n",
        "def write_document(\n",
        "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
        "    file_name: Annotated[str, \"File path to save the document.\"],\n",
        ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
        "    \"\"\"Create and save a text document.\"\"\"\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.write(content)\n",
        "    return f\"Document saved to {file_name}\"\n",
        "\n",
        "### Previous Complaint Data\n",
        "@tool \n",
        "def reference_previous_responses(\n",
        "    query: Annotated[str, \"The query to search for in the previous responses.\"],\n",
        ") -> Annotated[str, \"The previous responses that match the query.\"]:\n",
        "    \"\"\"Search for previous responses that match the query.\"\"\"\n",
        "    return qdrant_complaint_retriever.invoke(query)\n",
        "\n",
        "\n",
        "@tool\n",
        "def edit_document(\n",
        "    file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
        "    inserts: Annotated[\n",
        "        Dict[int, str],\n",
        "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
        "    ] = {},\n",
        ") -> Annotated[str, \"Path of the edited document file.\"]:\n",
        "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
        "\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    sorted_inserts = sorted(inserts.items())\n",
        "\n",
        "    for line_number, text in sorted_inserts:\n",
        "        if 1 <= line_number <= len(lines) + 1:\n",
        "            lines.insert(line_number - 1, text + \"\\n\")\n",
        "        else:\n",
        "            return f\"Error: Line number {line_number} is out of range.\"\n",
        "\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "    return f\"Document edited and saved to {file_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8yH1IAYK7nL"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "##### 🏗️ Activity #3:\n",
        "\n",
        "Describe, briefly, what each of these tools is doing in your own words.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer\n",
        "\n",
        "These tools work together to enable agents to create, refine, and manage documents in a structured, collaborative way. They allow for building content from scratch, referencing past work, and making precise improvements. Here’s what each does:\n",
        "\n",
        "- create_outline – Generates a numbered outline from key points and saves it to a file.\n",
        "\n",
        "- read_document – Opens a file and returns its content (full or by line range) for review.\n",
        "\n",
        "- write_document – Writes new content into a file and confirms where it’s saved.\n",
        "\n",
        "- reference_previous_responses – Retrieves past responses relevant to a query for consistency.\n",
        "\n",
        "- edit_document – Modifies an existing document by inserting text at specified line numbers.\n",
        "\n",
        "Together, these tools create a smooth workflow for drafting, reviewing, and improving documents, similar to how humans collaborate on writing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Jw_XBIFwwa"
      },
      "source": [
        "### Document Writing State\n",
        "\n",
        "Just like with our Research Team state - we want to keep track of a few things, however this time - we also want to keep track of which files we've created - so let's add that here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoU2YwJRu7wD"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from pathlib import Path\n",
        "\n",
        "class DocWritingState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: str\n",
        "    next: str\n",
        "    current_files: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p1kQShmGHCh"
      },
      "source": [
        "### Document Writing Prelude Function\n",
        "\n",
        "Since we have a working directory - we want to be clear about what our current working directory looks like - this helper function will allow us to do that cleanly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G79mUggQGLVq"
      },
      "outputs": [],
      "source": [
        "def prelude(state):\n",
        "    written_files = []\n",
        "    if not WORKING_DIRECTORY.exists():\n",
        "        WORKING_DIRECTORY.mkdir()\n",
        "    try:\n",
        "        written_files = [\n",
        "            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")\n",
        "        ]\n",
        "    except:\n",
        "        pass\n",
        "    if not written_files:\n",
        "        return {**state, \"current_files\": \"No files written.\"}\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n",
        "        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSre9agT9Gb"
      },
      "source": [
        "### Document Writing Node Creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7oso327T_wa"
      },
      "outputs": [],
      "source": [
        "doc_writer_agent = create_agent(\n",
        "    llm,\n",
        "    [write_document, edit_document, read_document],\n",
        "    (\"You are an expert writing responses to bill proposals.\\n\"\n",
        "    \"Below are files currently in your directory:\\n{current_files}\"),\n",
        ")\n",
        "context_aware_doc_writer_agent = prelude | doc_writer_agent\n",
        "doc_writing_node = functools.partial(\n",
        "    agent_node, agent=context_aware_doc_writer_agent, name=\"DocWriter\"\n",
        ")\n",
        "\n",
        "note_taking_agent = create_agent(\n",
        "    llm,\n",
        "    [create_outline, read_document, reference_previous_responses],\n",
        "    (\"You are an expert senior researcher tasked with writing a laws and bills and\"\n",
        "    \" taking notes to craft a response.\\n{current_files}\"),\n",
        ")\n",
        "context_aware_note_taking_agent = prelude | note_taking_agent\n",
        "note_taking_node = functools.partial(\n",
        "    agent_node, agent=context_aware_note_taking_agent, name=\"NoteTaker\"\n",
        ")\n",
        "\n",
        "copy_editor_agent = create_agent(\n",
        "    llm,\n",
        "    [write_document, edit_document, read_document],\n",
        "    (\"You are an expert copy editor who focuses on fixing grammar, spelling, and tone issues\\n\"\n",
        "    \"Below are files currently in your directory:\\n{current_files}\"),\n",
        ")\n",
        "context_aware_copy_editor_agent = prelude | copy_editor_agent\n",
        "copy_editing_node = functools.partial(\n",
        "    agent_node, agent=context_aware_copy_editor_agent, name=\"CopyEditor\"\n",
        ")\n",
        "\n",
        "empathy_editor_agent = create_agent(\n",
        "    llm,\n",
        "    [write_document, edit_document, read_document],\n",
        "    (\"You are an expert in empathy, compassion, and understanding - you edit the document to make sure it's empathetic and compassionate.\"\n",
        "    \"Below are files currently in your directory:\\n{current_files}\"),\n",
        ")\n",
        "empathy_editor_agent = prelude | empathy_editor_agent\n",
        "empathy_node = functools.partial(\n",
        "    agent_node, agent=empathy_editor_agent, name=\"EmpathyEditor\"\n",
        ")\n",
        "\n",
        "doc_writing_supervisor = create_team_supervisor(\n",
        "    llm,\n",
        "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers: {team_members}. You should always verify the technical\"\n",
        "    \" contents after any edits are made. \"\n",
        "    \"Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When each team is finished,\"\n",
        "    \" you must respond with FINISH.\"),\n",
        "    [\"DocWriter\", \"NoteTaker\", \"EmpathyEditor\", \"CopyEditor\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUiNMpJBGXN0"
      },
      "source": [
        "### Document Writing Team LangGraph Construction\n",
        "\n",
        "This part is almost exactly the same (with a few extra nodes) as our Research Team LangGraph construction - so we'll leave it as one block!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6n8A1ytxVTv"
      },
      "outputs": [],
      "source": [
        "authoring_graph = StateGraph(DocWritingState)\n",
        "authoring_graph.add_node(\"DocWriter\", doc_writing_node)\n",
        "authoring_graph.add_node(\"NoteTaker\", note_taking_node)\n",
        "authoring_graph.add_node(\"CopyEditor\", copy_editing_node)\n",
        "authoring_graph.add_node(\"EmpathyEditor\", empathy_node)\n",
        "authoring_graph.add_node(\"supervisor\", doc_writing_supervisor)\n",
        "\n",
        "authoring_graph.add_edge(\"DocWriter\", \"supervisor\")\n",
        "authoring_graph.add_edge(\"NoteTaker\", \"supervisor\")\n",
        "authoring_graph.add_edge(\"CopyEditor\", \"supervisor\")\n",
        "authoring_graph.add_edge(\"EmpathyEditor\", \"supervisor\")\n",
        "\n",
        "authoring_graph.add_conditional_edges(\n",
        "    \"supervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"DocWriter\": \"DocWriter\",\n",
        "        \"NoteTaker\": \"NoteTaker\",\n",
        "        \"CopyEditor\" : \"CopyEditor\",\n",
        "        \"EmpathyEditor\" : \"EmpathyEditor\",\n",
        "        \"FINISH\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "authoring_graph.set_entry_point(\"supervisor\")\n",
        "compiled_authoring_graph = authoring_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-EKGkHKUBO"
      },
      "source": [
        "#### Display Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "AZdOb3GZKSM7",
        "outputId": "6b64588d-5568-4234-d062-4dc83ea9abec"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        compiled_authoring_graph.get_graph().draw_mermaid_png(\n",
        "            curve_style=CurveStyle.LINEAR,\n",
        "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
        "            wrap_label_n_words=9,\n",
        "            output_file_path=None,\n",
        "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
        "            background_color=\"white\",\n",
        "            padding=10,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB_rOw1hGpwd"
      },
      "source": [
        "Just as before - we'll need to create an \"interface\" between the level above, and our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-RbbCKoG_nt"
      },
      "outputs": [],
      "source": [
        "def enter_chain(message: str, members: List[str]):\n",
        "    results = {\n",
        "        \"messages\": [HumanMessage(content=message)],\n",
        "        \"team_members\": \", \".join(members),\n",
        "    }\n",
        "    return results\n",
        "\n",
        "authoring_chain = (\n",
        "    functools.partial(enter_chain, members=authoring_graph.nodes)\n",
        "    | authoring_graph.compile()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgyhpTrRNgQd"
      },
      "source": [
        "Now we can test this out!\n",
        "\n",
        "> NOTE: It is possible you may see an error here - rerun the cell to clear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWUxv4XDx3kg",
        "outputId": "62ee7d3d-31ba-4348-b852-7fd96f6875ff"
      },
      "outputs": [],
      "source": [
        "for s in authoring_chain.stream(\n",
        "    \"Write a response on the position of AI bills in Philippines and how it can affect the research and development of AI in the country.\",\n",
        "    {\"recursion_limit\": 100},\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpW2R9SUHGUq"
      },
      "source": [
        "## Task 5: Meta-Supervisor and Full Graph\n",
        "\n",
        "Finally, now that we have our two LangGraph agents (some of which are already multi-agent), we can build a supervisor that sits above all of them!\n",
        "\n",
        "The final process, surprisingly, is quite straight forward!\n",
        "\n",
        "Let's jump in!\n",
        "\n",
        "First off - we'll need to create our supervisor agent node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkpxeUf9ygKp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "supervisor_node = create_team_supervisor(\n",
        "    llm,\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following teams: {team_members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When all workers are finished,\"\n",
        "    \" you must respond with FINISH.\",\n",
        "    [\"Research team\", \"Response team\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUvOh_xWIKig"
      },
      "source": [
        "We'll also create our new state - as well as some methods to help us navigate the new state and the subgraphs.\n",
        "\n",
        "> NOTE: We only pass the most recent message from the parent graph to the subgraph, and we only extract the most recent message from the subgraph to include in the state of the parent graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7HJ8MF0yh_i"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    next: str\n",
        "\n",
        "def get_last_message(state: State) -> str:\n",
        "    return state[\"messages\"][-1].content\n",
        "\n",
        "def join_graph(response: dict):\n",
        "    return {\"messages\": [response[\"messages\"][-1]]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RHao1sIanG"
      },
      "source": [
        "Next, we'll create our base graph.\n",
        "\n",
        "Notice how each node we're adding is *AN ENTIRE LANGGRAPH AGENT* (wrapped into an LCEL chain with our helper functions above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfCWABCMIaFy"
      },
      "outputs": [],
      "source": [
        "super_graph = StateGraph(State)\n",
        "\n",
        "super_graph.add_node(\"Research team\", get_last_message | research_chain | join_graph)\n",
        "super_graph.add_node(\"Response team\", get_last_message | authoring_chain | join_graph)\n",
        "super_graph.add_node(\"supervisor\", supervisor_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpwpUXMtI62E"
      },
      "source": [
        "Next, we'll create our edges!\n",
        "\n",
        "This process is completely idenctical to what we've seen before - just addressing the LangGraph subgraph nodes instead of individual nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLtjRuUYI-fx"
      },
      "outputs": [],
      "source": [
        "super_graph.add_edge(\"Research team\", \"supervisor\")\n",
        "super_graph.add_edge(\"Response team\", \"supervisor\")\n",
        "super_graph.add_conditional_edges(\n",
        "    \"supervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"Response team\": \"Response team\",\n",
        "        \"Research team\": \"Research team\",\n",
        "        \"FINISH\": END,\n",
        "    },\n",
        ")\n",
        "super_graph.set_entry_point(\"supervisor\")\n",
        "compiled_super_graph = super_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1KMfFqgJKw8"
      },
      "source": [
        "That's it!\n",
        "\n",
        "Now we can finally use our full agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M6wUDR-yk8s",
        "outputId": "056fe89e-5a81-4852-f0cb-35367da8cef0"
      },
      "outputs": [],
      "source": [
        "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
        "\n",
        "for s in compiled_super_graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Write a statement on the positioning of AI bills in Philippines as it relates to impact of penalties and listing of directories to AI research anddevelopment in the country. Is it feasible to implement a directory of all AI applications or will it slow down progress? Conduct research against global AI bills and how other countries are handling AI bills. Then make sure you consult the response team, and check for copy editing and dopeness, and write the file to disk.\"\n",
        "            )\n",
        "        ],\n",
        "    },\n",
        "    {\"recursion_limit\": 30},\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuZAvSlJJpPP"
      },
      "source": [
        "## SAMPLE POST!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOEMCrXTJaxW"
      },
      "source": [
        "The regulatory landscape for artificial intelligence (AI) in the Philippines is undergoing significant evolution through the introduction of several bills designed to manage the development and implementation of AI technologies. These proposed legislations highlight the government’s awareness of AI's potential to profoundly enhance various sectors, including public services, disaster resilience, agriculture, education, and healthcare.  \n",
        "\n",
        "For AI developers, these bills offer a framework that aims to balance technological innovation with ethical considerations and public safety. The regulations stress the importance of transparency, accountability, and human oversight in AI development, serving as safeguards against inherent risks such as algorithmic bias and misinformation. Notably, the legislation draws inspiration from global standards, such as the European Union's AI Act, advocating for a regulatory approach that promotes responsible AI innovation without hindering creativity and progress.  \n",
        "\n",
        "Moreover, the bills establish the state as a protector of citizens' rights, ensuring that AI is developed and utilized in ways that minimize harm and prevent abuse. By setting forth these guidelines, the legislation aspires to cultivate an environment that supports the creativity and ingenuity of Filipino developers while aligning with global best practices.  \n",
        "\n",
        "In conclusion, the proposed AI bills in the Philippines represent a commitment to fostering a responsible, ethical, and forward-thinking approach to artificial intelligence. This dual focus on innovation and regulation is vital for encouraging development while safeguarding the well-being of the population and society as a whole."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
